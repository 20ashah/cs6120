+++
title = "Dynamic Compilers"
draft = true
[extra]
due = "November 23"
index = 11
[[extra.readings]]
name = "talk video on the history of JITs"
url = "https://youtu.be/tWvaSkgVPpA"
details = "by JF Bastien at CppCon 2020"
+++
## Gist

### Motivation

So far, the compilers you have written in 6120 have been *ahead-of-time* (AOT) or *static* compilers: they compile the code to some other form in one phase, and then execution happens later—and has nothing to do with the compiler.
*Just-in-time* (JIT) or *dynamic* compilers mix together compilation and execution: they translate your code when it's time to run it, so they are intimately involved in program execution.

Why would you want a dynamic compiler? There are two main reasons:

* JITs have access to information that static compilers do not. They can compile programs knowing something about how often a given branch is taken, which type a variable has, or even detailed statistics about the inputs the program is dealing with. In other words, JITs get to do *profile-guided optimization*, which is also possible in the AOT world but less convenient (because it requires an extra, slow-running phase during compilation) and less accurate (because it relies on the assumption that the inputs and program behavior during profiling are close to the behavior in deployment).
* Some kinds of languages *really* benefit from exploiting that run-time information. In particular, [dynamic languages][dl] like JavaScript, Python, and Ruby are absurdly popular—and their semantics make it really hard for AOT compilers to produce efficient code. For example, the expression `a + b` in Python in general requires looking up and invoking up a method called [`__add__`][pyadd] in the object `a`. If we could somehow know that `a` and `b` were both `float` values, we could conceivably implement it instead with a single CPU instruction. But Python makes it very hard to know statically what the types of variables are. A JIT compiler can profile the actual dynamic execution to determine what these types are and then use them to generate far better code.

Here are some semi-related terminology rants:

* You may have heard of something called "interpreted languages."
  There is no such thing: languages do not have an intrinsic property that makes them interpreted or compiled.
  You can implement Rust in an interpreter, Python with an AOT compiler, or C++ with a JIT.
  When people say "interpreted language," they usually mean "a language for which it is hard to write a high-quality compiler."
* Interpretation vs. compilation is a spectrum.
  For example, we usually think of [CPython][] as an interpreter rather than a compiler, but it actually has a compilation step:
  it translates Python programs into a more regular [bytecode][py-bytecode] representation and then interprets that, which is both simpler and more efficient than interpreting the full Python AST.
  That's quite different, of course, from a full-blown JIT like [PyPy][] that translates even farther to actual machine instructions.
  While plain interpreters are common in CS homework, they are pretty rare in the real world—bytecode interpreters with a first-stage JIT compiler are much more common.

### Anatomy of a JIT

Here's what you need to make a basic JIT:

* An interpreter (or another "cheap" way to run the code, like a simple non-optimizing compiler). Your JIT will start the lifecycle by interpreting the code and collecting basic statistics, like the number of times a function or loop body runs.
* A heuristic to detect "hotness." When your JIT finds that a function or loop or something is hot (i.e., seems to run a lot), it's time to do more optimization.
* A profiler. When you decide that some code is worth optimizing farther, you can afford to spend some time collecting more information about its execution that you can use during optimization. This is usually a different mode in your interpreter that runs more slowly but collects more detailed information, like the types of values and the directions of every branch.
* An optimizing compiler. This component can be just like any AOT compiler, except that it also gets to use the information from the profiling step. Usually, though, you will want to generate code that checks that the assumptions from the profiler still hold: that the types are still the same as during profiling, for example. These checks are often called *guards*.
* Ways to switch between interpretation and execution of compiled code.
    * Your JIT needs to detect when the interpreter reaches a function that has previously been compiled, for example, and call that pre-compiled code instead.
    * And when compiled code's guards fail (when the compiled code relies on assumptions that eventually do not hold), the JIT needs to switch back to interpretation. Falling back from the compiler to the interpreter is called *deoptimization*.
    * Switching between the two modes of execution requires somehow mapping between the different formats they use for program state in a process called *on-stack replacement* if it can happen in the middle of a function execution.

Refining things further, there are two main strategies for JIT compilation:

* *Method JITs* are the "normal" kind: they compile a function at a time, once they determine that the function has become hot enough. (I don't really know how these got that name instead of "function JIT," which would be much more sensible. But I think it's probably because of how closely associated JIT compilation and Java, which insists on always calling them "methods.")
* *Tracing JITs* are very different: they extract possibly-interprocedural hot *paths* through the program's control flow and compile those.

### A Bit More on Tracing

How to trace:

- Just run every instruction.
- Sometimes you have concrete values; sometimes you don't.
- When you branch, turn it into an assertion. So after going into `if (x==2)`, you can update your heap so x=2.

## Tasks

The task is to implement a *tracer* for Bril in the reference interpreter.
Your extended interpreter should run a program and, when execution finishes, produce an equivalent program that uses Bril's speculation extension to run some cast on a "fast path."

Here's a recipe:

* Start interpreting normally.
* At some point during execution (at the very beginning of `main`, for example, or when reaching a backedge), start tracing.
* While tracing, record every instruction as it executes. Eliminate jumps; replace branches with `guard` instructions.
* Stop tracing at some point (after a fixed number of instructions, for example, or at the next backedge).
* For bonus "points," optimize the trace by eliminating instructions that depend on foregone conclusions enforced by guards.
* Stitch the trace back into the program using `speculate` and `resolve` instructions.
* Convince yourself that your tracing is correct by running the Bril benchmarks on both old and new inputs.

[dl]: https://en.wikipedia.org/wiki/Dynamic_programming_language
[pyadd]: https://docs.python.org/3/reference/datamodel.html#object.__add__
[cpython]: https://github.com/python/cpython
[py-bytecode]: https://docs.python.org/3/library/dis.html
[pypy]: https://www.pypy.org
