+++
title = "Finding Redundant Structures in Data Flow Graphs"
[extra]
bio = """
  [Oliver][] is a CS PhD student in theory at Cornell, who does decision theory and category theory.

  [Alexa][] is a second-year student interested in the intersection of compilers and formal methods. She also enjoys feminist book clubs and cooking elaborate [fish truck][] meals.

  [Greg][] is a second-year student working on machine learning and digital humanities.

[alexa]: https://www.cs.cornell.edu/~avh
[greg]: https://www.cs.cornell.edu/~gyauney
[oliver]: https://www.cs.cornell.edu/~oli
[fish truck]: https://www.triphammermarketplace.com/events/
"""
latex = true

[[extra.authors]]
name = "Oliver Richardson"
link = "https://www.cs.cornell.edu/~oli"
[[extra.authors]]
name = "Alexa VanHattum"
link = "https://www.cs.cornell.edu/~avh"
[[extra.authors]]
name = "Gregory Yauney"
link = "https://www.cs.cornell.edu/~gyauney"
+++

In a conventional [von Neumann architecture][vn], we might think of computation
at a high level as as our computers faithfully carrying out a series of steps.
Like dominoes in a line, the program counter runs through each instruction once
its predecessor completes.

[vn]: https://en.wikipedia.org/wiki/Von_Neumann_architecture


<img src="dominos.gif" width=60%/>


Of course, this mental model is far from the truth—in modern [out-of-order][ooo]
processors, instructions are aggressively reordered to take advantage of
multiple processing elements at once. The major caveat here is that reordering
must respect the original program's _data flow_. That is, if an instruction
needs to use data that is generated by a previous instruction, then it cannot be
reordered to happen afterward. From this perspective, we can think of
computation as dictating the flow of data through operations. Each instruction
is a node, and dependencies form edges that flows along. These dependencies form
a [_data flow graph_ (DFG)][dfg] for the program.

[ooo]: https://en.wikipedia.org/wiki/Out-of-order_execution
[dfg]: https://en.wikipedia.org/wiki/Data-flow_analysis

<img src="pipes.gif" width=50%/>

## Go with the flow

Analyzing the data flow graphs of programs allows us to think about the _shape_
of the computation, independent of the literal order a programmer used
to specify it. In particular, two separate programs are more likely to share
data flow structure than literal source code redundancy (since many reorderings
can maintain the same data flow). Even within the same source program, shared
structure in the data flow graph may indicate core computational patterns.

### Data flow graphs for computational acceleration

If our goal is to compile faster or more energy-efficient code, data flow graphs
can help show us where to focus. By identifying redundant subgraphs in the
structure of data flow graphs, we can find groupings of operations that we
expect to occur frequently enough to benefit from additional optimization
effort. What's more, the shape of the subgraphs is also a signal for how
_useful_ the acceleration might be: subgraphs that are wider, rather than simply
linear chains, indicate more opportunity for [_fine-grained parallelism_][fgp].
Our goals in this project are shaped by the domain of hardware acceleration with
[_heterogeneous computing_][hc], where a compiler's goal is to target multiple
processors, each with differing strengths and weaknesses.

For this project, we build on the [LLVM compiler infrastructure][llvm] to find
redundant structures in programs' static data flow graphs. Our goal is to find a
fixed number of subgraph structures that occur the most frequently (that is,
cover the highest number of instructions) throughout the program. We focus on
finding candidate subgraphs with high frequency, and leave analysis and
heterogeneous compilation of those subgraphs to later work. The source code for
this project and our profiling analysis can be found [here][source].

[fgp]: https://en.wikipedia.org/wiki/Granularity_(parallel_computing)#Fine-grained_parallelism
[hc]: https://en.wikipedia.org/wiki/Heterogeneous_computing
[llvm]: https://llvm.org
[source]: https://github.com/avanhatt/dfg-coverings

## Building data flow graphs from LLVM

Data flow graphs exist at multiple levels of abstraction in a compiler
toolchain, and there are trade-offs to targeting any particular choice.

First, data flow graphs can either represent a program _statically_, purely from
the program's source code, or _dynamically_, from a program execution trace. A
static DFG has a one-to-one relation to the source code: each operation and its
dependencies are directly translated. The control flow of the program exists
only implicitly: if a data value's flow depends on the branching structure of
the program, the DFG would have back edges and cycles. A dynamic DFG captures a
single trace throughout the program, where operations are repeated each time
they are executed. In this case, the data flow graph remains acyclic (with
values only flowing "down"), and loops in the control flow repeat in the
subgraph for each time the loop is executed. However, dynamic data flow graphs
only represent a single execution of the program, and may not even cover the
full program behavior. They also may be infeasible to generate ahead of time
for long-running applications.

In addition, DFGs can target either the _intermediate representation_ level,
with LLVM-level operations, or at the _machine code_ level, with operations
corresponding to the exact instruction set architecture. The machine code
data flow graph corresponds more directly to the program's actual execution, but
is not as general across different targets.

For this project, we use LLVM to target the static DFG at the intermediate
representation level of abstraction. LLVM translates the program source to
[_static single assignment (SSA)_][ssa] form, where every variable name can only
be assigned to once. Thus, each instruction represents one operation. Because
LLVM's in memory intermediate representation stores pointers to instructions'
operands, we can build a program's static data flow graph by inserting edges
to an instruction and from each of its operands. We narrow the project's scope
to only consider acyclic subgraphs by considering subgraphs only within basic
block boundaries, which lack branching control flow.

[ssa]: https://en.wikipedia.org/wiki/Static_single_assignment_form

## Matching DFG stencils

From a modeling perspective, a stencil is more than just the topology of the graph: a stencil also includes the class of operation for each node. For instance, consider stencil formed by the chain of instructions `pointer -> getelementptr -> load` --- the load instruction cannot be mapped arbitrarily onto other instructions: we want it to align only with program instructions which are in some sense the same. Thinking of the opcodes as each specifying a color, this makes a stencil a colored graph, and a stencil isomorphism is a bijection of colored graphs.

While graph isomorphism is a notoriously tricky problem, it is also a very common one, and we make heavy use of the `networkx.isomorphism` package, which provides tools for iterating over matches (colored graph isomorphisms) between program instructions $G$ and a stencil $H$.

We started our testing with the following hand-picked chains of instructions extracted from the `embench/matmult-int` code:

```
	chains = [
		["mul", "add", "srem"],
		["shl", "add"],
		["sdiv", "mul", "add"],
		["load", "mul"],
	]
```

Though it was never our goal to end here, it quickly became apparent that this was not going to be even a little bit effective, and would be very overfit to the program we were looking at. The original program, `matmult-int`, only matched ~4% of instructions, and other programs, such as `add.c` did not match a single one of them.


## Generating common DFG stencils

Of course, doing this by hand is tedious and not particularly effective; we would like to automate the process of finding the stencils to accelerate.

We have implemented this in two different ways TODO

### Formal Description of the Task

If we ignore control flow, we can look at the problem purely graph theoretically. For a single trace through the program, the data flow graph $G$ is acyclic, and we would like to cover as much of it as possible with sub-graphs, corresponding to the stencils that we accelerate.
Statically, we do not know what the final data flow graph is, but we do know that we will be able to assemble one by connecting dangling edges from control-flow-free components: basic blocks.
This is the approach we take.

We would like to find a small collection of graph components $\mathcal H = \{H_i, \ldots, H_k\}$, which we can use to replace parts of and accelerate programs having basic blocks $\mathcal G = \{G_1,\ldots,G_n\}$, that maximizes the total saved time:

$$\mathcal S_{\mathcal H}(\mathcal G) := \max_{\mathcal C \in \text{Cov}(\mathcal G, \mathcal H)}~ \sum_{G \in \mathcal G} w_G \cdot  \sum_{H \in \mathcal C_G} f_H \cdot |H|$$

where:

- $\text{Cov}(\mathcal G, \mathcal H)$ is the set of all valid (partial) coverings of basic blocks with at most one stencil, that is, injective graph morphisms $\varphi: (\cup \mathcal G) \to \cup \mathcal H$.

- $\mathcal C_G$ is the component of the total covering on the particular basic block graph $G$.

- $w_G$ is independent of $\mathcal H$ and proportional to the expected number of times $G$ is executed.

- $f_H$ is the expected speedup factor from accelerating the component $H$.

Of course, supposing that $f_H$ was roughly constant, we could trivially achieve the maximum savings by choosing $\mathcal H := \mathcal G$, there are a few problems with this:

1.  $|\mathcal H|$ is large; there are many of these sub-graphs, which makes the search process substantially less efficient.
2. Each $H_i \in \mathcal H$ is also large, making the specialized component more expensive.
3. There is now a dependency between $\mathcal H$ and $\mathcal G$, and so we need to know our program in order to build the components we use to accelerate.


In fact, only the third issue is really important; the first two are roughly heuristics which help solve it. To cast this as a learning problem, imagine that there's some underlying distribution $\mathtt{Programs}$ of programs that people write; we can now cast our work as a solution to the optimization problem of finding

$$ \arg\max_{\mathcal H}\left( \mathop{\mathbb E}\limits_{\mathcal G\sim \texttt{Programs}}~ \mathcal S_{\mathcal H}(\mathcal G) - \text{Cost}(\mathcal H) \right)$$

where $\text{Cost}(\mathcal H)$ is the additional compilation cost incurred by $\mathcal H$, which is higher for larger graphs.
Rather than solve this optimization problem in closed form, we optimize for heuristics (1) and (2), exposing knobs that could be used in future work to automate the entire optimization.

### Edge Subgaphs

### Node Sub-graphs

- n-node vs. n-edge stencils
- Beam search
- Scaling

## Static and dynamic coverage

- Annotations on LLVM
- Embench benchmark suite
- Use fast stencils for slow/big applications

## Ongoing directions

While finding redundancies in DFGs within each basic block is a good initial
approach, this project could be extended in several directions.

We could build on existing literature in [extended basic blocks][ebb] to find
subgraphs that _speculatively_ occur. That is, in extended basic blocks, we
consider control flows that are likely to jump from one block to another in the
common case, and only fall back to different branches in the case that our guess
of the next block was wrong. In the context of hardware acceleration, we can
imagine building accelerators that handle these larger speculative subgraphs
when possible, and fall back to slower CPU execution if the control flow
differs.

In addition, it would be interesting to compare this project against dynamic
data flow graphs. For example, the [Redux][] paper essentially introduced the
formulation of dynamic data flow graphs as we desribe them here, and outlines
how to generate efficiently generate them. From the perspective of hardware
acceleration, the [RADISH][] paper (“Iterative Search for Reconfigurable
Accelerator Blocks with a Compiler in the Loop”) uses Python wrappers to
generate dynamic data flow graphs, and heuristic genetic algorithms to "fuse"
similar dynamic graphs together.

Like RADISH, we could extend our application to target _groups_ of applications
instead of single programs. The scale of this undertaking would require more
clever heuristics than our current search strategies, but would ideally help us
find more general subgraphs to accelerate.

Finally, the impact of this project could be more clearly explicated by
evaluating our subgraph identification with actual computational acceleration.
In particular, we hope this strategy will prove useful in conjunction with other
work that aims use compile-time analysis to target heterogeneous targets.

[ebb]: https://en.wikipedia.org/wiki/Extended_basic_block
[redux]: http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7CE631B431BCCBA459061BC458D53E8F?doi=10.1.1.63.2083&rep=rep1&type=pdf
[RADISH]: https://ieeexplore.ieee.org/document/8509181
