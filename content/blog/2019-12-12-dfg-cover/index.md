+++
title = "Finding Redundant Structures in Data Flow Graphs"
[extra]
bio = """
  [Oliver][] is a CS PhD student in theory at Cornell, who does decision theory and category theory.

  [Alexa][] is a second-year student interested in the intersection of compilers and formal methods. She also enjoys feminist book clubs and cooking elaborate [fish truck][] meals.

  [Greg][] is a second-year student working on machine learning and digital humanities.

[alexa]: https://www.cs.cornell.edu/~avh
[greg]: https://www.cs.cornell.edu/~gyauney
[oliver]: https://www.cs.cornell.edu/~oli
[fish truck]: https://www.triphammermarketplace.com/events/
"""
latex = true

[[extra.authors]]
name = "Oliver Richardson"
link = "https://www.cs.cornell.edu/~oli"
[[extra.authors]]
name = "Alexa VanHattum"
link = "https://www.cs.cornell.edu/~avh"
[[extra.authors]]
name = "Gregory Yauney"
link = "https://www.cs.cornell.edu/~gyauney"
+++

In a conventional von Neumann architecture, we might think of computation at a
high level as as our computers faithfully carrying out a series of steps. Like
dominoes in a line, the program counter runs through each instruction once its
predecessor completes.


<img src="dominos.gif" width=60%/>


Of course, this mental model is far from the truthâ€”in modern out-of-order
processors, instructions are aggressively reordered to take advantage of
multiple processing elements at once. The major caveat here is that reordering
must respect the original program's _data flow_. That is, if an instruction
needs to use data that is generated by a previous instruction, then it cannot be
reordered to happen afterward. From this perspective, we can think of
computation as dictating the flow of data through operations. Each instruction
is a node, and dependencies form edges that flows along.

<img src="pipes.gif" width=50%/>


## Go with the flow


Fun introduction!

## Data flow graphs for computational acceleration

- Dependencies matter!
- DFGs nicely model spatial acceleration

## Formal Description of the Task

If we ignore control flow, we can look at the problem purely graph theoretically. For a single trace through the program, the data flow graph $G$ is acyclic, and we would like to cover as much of it as possible with sub-graphs, corresponding to the stencils that we accelerate.
Statically, we do not know what the final data flow graph is, but we do know that we will be able to assemble one by connecting dangling edges from control-flow-free components: basic blocks.
This is the approach we take.  

We would like to find a small collection of graph components $\mathcal H = \{H_i, \ldots, H_k\}$, which we can use to replace parts of and accelerate programs having basic blocks $\mathcal G = \{G_1,\ldots,G_n\}$, that maximizes the total saved time:

$$\mathcal S_{\mathcal H}(\mathcal G) := \max_{\mathcal C \in \text{Cov}(\mathcal G, \mathcal H)}~ \sum_{G \in \mathcal G} w_G \cdot  \sum_{H \in \mathcal C_G} f_H \cdot |H|$$

where:

- $\text{Cov}(\mathcal G, \mathcal H)$ is the set of all valid (partial) coverings of basic blocks with at most one stencil, that is, injective graph morphisms $\varphi: (\cup \mathcal G) \to \cup \mathcal H$.

- $\mathcal C_G$ is the component of the total covering on the particular basic block graph $G$.

- $w_G$ is independent of $\mathcal H$ and proportional to the expected number of times $G$ is executed.

- $f_H$ is the expected speedup factor from accelerating the component $H$.

Of course, supposing that $f_H$ was roughly constant, we could trivially achieve the maximum savings by choosing $\mathcal H := \mathcal G$, there are a few problems with this:

1.  $|\mathcal H|$ is large; there are many of these sub-graphs, which makes the search process substantially less efficient.
2. Each $H_i \in \mathcal H$ is also large, making the specialized component more expensive.
3. There is now a dependency between $\mathcal H$ and $\mathcal G$, and so we need to know our program in order to build the components we use to accelerate.


In fact, only the third issue is really important; the first two are roughly heuristics which help solve it. To cast this as a learning problem, imagine that there's some underlying distribution $\mathtt{Programs}$ of programs that people write; we can now cast our work as a solution to the optimization problem of finding

$$ \arg\max_{\mathcal H}\left( \mathop{\mathbb E}\limits_{\mathcal G\sim \texttt{Programs}}~ \mathcal S_{\mathcal H}(\mathcal G) - \text{Cost}(\mathcal H) \right)$$

where $\text{Cost}(\mathcal H)$ is the additional compilation cost incurred by $\mathcal H$, which is higher for larger graphs.
Rather than solve this optimization problem in closed form, we optimize for heuristics (1) and (2), exposing knobs that could be used in future work to automate the entire optimization.


## Building data flow graphs from LLVM

- Trade-offs:
- Machine instructions vs. IR instructions
- Static vs. dynamic DFGs
- Getting simple data flow "for free" vs. complexities of control flow

## Matching fixed DFG stencils

- Defining node matches
- Finding isomorphisms

## Generating common DFG stencils

Of course, doing this by hand is tedious and not particularly effective; we would like to automate the process of finding the stencils to accelerate.

We have implemented this in two different ways


###

### Node Sub-graphs


- n-node vs. n-edge stencils
- Beam search
- Scaling

## Static and dynamic coverage

- Annotations on LLVM
- Embench benchmark suite
- Use fast stencils for slow/big applications

## Ongoing directions

- Extend to hyperblock/superblock
- Compare against dynamic DFGs
- Evaluate on accelerated hardware
- Find stencils for groups of applications
