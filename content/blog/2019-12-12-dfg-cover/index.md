+++
title = "Finding Redundant Structures in Data Flow Graphs"
[extra]
bio = """
  [Oliver][] is a CS PhD student in theory at Cornell, who does decision theory and category theory.

  [Alexa][] is a second-year student interested in the intersection of compilers and formal methods. She also enjoys feminist book clubs and cooking elaborate [fish truck][] meals.

  [Greg][] is a second-year student working on machine learning and digital humanities.

[alexa]: https://www.cs.cornell.edu/~avh
[greg]: https://www.cs.cornell.edu/~gyauney
[oliver]: https://www.cs.cornell.edu/~oli
[fish truck]: https://www.triphammermarketplace.com/events/
"""
latex = true

[[extra.authors]]
name = "Oliver Richardson"
link = "https://www.cs.cornell.edu/~oli"
[[extra.authors]]
name = "Alexa VanHattum"
link = "https://www.cs.cornell.edu/~avh"
[[extra.authors]]
name = "Gregory Yauney"
link = "https://www.cs.cornell.edu/~gyauney"
+++

In a conventional [von Neumann architecture][vn], we might think of computation
at a high level as as our computers faithfully carrying out a series of steps.
Like dominoes in a line, the program counter runs through each instruction once
its predecessor completes.

[vn]: https://en.wikipedia.org/wiki/Von_Neumann_architecture


<img src="dominos.gif" width=60%/>


Of course, this mental model is far from the truthâ€”in modern [out-of-order][ooo]
processors, instructions are aggressively reordered to take advantage of
multiple processing elements at once. The major caveat here is that reordering
must respect the original program's _data flow_. That is, if an instruction
needs to use data that is generated by a previous instruction, then it cannot be
reordered to happen afterward. From this perspective, we can think of
computation as dictating the flow of data through operations. Each instruction
is a node, and dependencies form edges that flows along. These dependencies form
a [_data flow graph_ (DFG)][dfg] for the program.

[ooo]: https://en.wikipedia.org/wiki/Out-of-order_execution
[dfg]: https://en.wikipedia.org/wiki/Data-flow_analysis

<img src="pipes.gif" width=50%/>

## Go with the flow

Analyzing the data flow graphs of programs allows us to think about the _shape_
of the computation, independent of the literal order a programmer used
to specify it. In particular, two separate programs are more likely to share
data flow structure than literal source code redundancy (since many reorderings
can maintain the same data flow). Even within the same source program, shared
structure in the data flow graph may indicate core computational patterns.

### Data flow graphs for computational acceleration

If our goal is to compile faster or more energy-efficient code, data flow graphs
can help show us where to focus. By identifying redundant subgraphs in the
structure of data flow graphs, we can find groupings of operations that we
expect to occur frequently enough to benefit from hardware acceleration. What's
more, the shape of the subgraphs is also a signal for how _useful_ the
acceleration might be: subgraphs that are wider, rather than simply linear
chains, indicate more opportunity for [_fine-grained parallelism_][fgp].

For this project, we build on the [LLVM compiler infrastructure][llvm] to find
redundant structures in programs' static data flow graphs. Our goal is to find a
fixed number of subgraph structures that occur the most frequently (that is,
cover the highest number of instructions) throughout the program. We focus on
finding candidate subgraphs with high frequency, and leave analysis of the shape
of those subgraphs to later work. The source code for this project and our
profiling analysis can be found [here][source].

[fgp]: https://en.wikipedia.org/wiki/Granularity_(parallel_computing)#Fine-grained_parallelism
[llvm]: https://llvm.org
[source]: https://github.com/avanhatt/dfg-coverings

## Building data flow graphs from LLVM

- Trade-offs:
- Machine instructions vs. IR instructions
- Static vs. dynamic DFGs
- Getting simple data flow "for free" vs. complexities of control flow

## Matching fixed DFG stencils

- Defining node matches
- Finding isomorphisms

## Generating common DFG stencils

- n-node vs. n-edge stencils
- Beam search
- Scaling

## Static and dynamic coverage

- Annotations on LLVM
- Embench benchmark suite
- Use fast stencils for slow/big applications

## Ongoing directions
- Extend to hyperblock/superblock
- Compare against dynamic DFGs
- Evaluate on accelerated hardware
- Find stencils for groups of applications
