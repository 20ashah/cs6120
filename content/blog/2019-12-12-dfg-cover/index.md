+++
title = "Finding Redundant Structures in Data Flow Graphs"
[extra]
bio = """
  [Oliver][] is a CS PhD student in theory at Cornell, who does decision theory and category theory.

  [Alexa][] is a second-year student interested in the intersection of compilers and formal methods. She also enjoys feminist book clubs and cooking elaborate [fish truck][] meals.

  [Greg][] is a second-year student working on machine learning and digital humanities.

[alexa]: https://www.cs.cornell.edu/~avh
[greg]: https://www.cs.cornell.edu/~gyauney
[oliver]: https://www.cs.cornell.edu/~oli
[fish truck]: https://www.triphammermarketplace.com/events/
"""
latex = true

[[extra.authors]]
name = "Oliver Richardson"
link = "https://www.cs.cornell.edu/~oli"
[[extra.authors]]
name = "Alexa VanHattum"
link = "https://www.cs.cornell.edu/~avh"
[[extra.authors]]
name = "Gregory Yauney"
link = "https://www.cs.cornell.edu/~gyauney"
+++

In a conventional [von Neumann architecture][vn], we might think of computation
at a high level as as our computers faithfully carrying out a series of steps.
Like dominoes in a line, the program counter runs through each instruction once
its predecessor completes.

[vn]: https://en.wikipedia.org/wiki/Von_Neumann_architecture


<img src="dominos.gif" width=60%/>


Of course, this mental model is far from the truthâ€”in modern [out-of-order][ooo]
processors, instructions are aggressively reordered to take advantage of
multiple processing elements at once. The major caveat here is that reordering
must respect the original program's _data flow_. That is, if an instruction
needs to use data that is generated by a previous instruction, then it cannot be
reordered to happen afterward. From this perspective, we can think of
computation as dictating the flow of data through operations. Each instruction
is a node, and dependencies form edges that flows along. These dependencies form
a [_data flow graph_ (DFG)][dfg] for the program.

[ooo]: https://en.wikipedia.org/wiki/Out-of-order_execution
[dfg]: https://en.wikipedia.org/wiki/Data-flow_analysis

<img src="pipes.gif" width=50%/>

## Go with the flow

Analyzing the data flow graphs of programs allows us to think about the _shape_
of the computation, independent of the literal order a programmer used
to specify it. In particular, two separate programs are more likely to share
data flow structure than literal source code redundancy (since many reorderings
can maintain the same data flow). Even within the same source program, shared
structure in the data flow graph may indicate core computational patterns.

### Data flow graphs for computational acceleration

If our goal is to compile faster or more energy-efficient code, data flow graphs
can help show us where to focus. By identifying redundant subgraphs in the
structure of data flow graphs, we can find groupings of operations that we
expect to occur frequently enough to benefit from hardware acceleration. What's
more, the shape of the subgraphs is also a signal for how _useful_ the
acceleration might be: subgraphs that are wider, rather than simply linear
chains, indicate more opportunity for [_fine-grained parallelism_][fgp].

For this project, we build on the [LLVM compiler infrastructure][llvm] to find
redundant structures in programs' static data flow graphs. Our goal is to find a
fixed number of subgraph structures that occur the most frequently (that is,
cover the highest number of instructions) throughout the program. We focus on
finding candidate subgraphs with high frequency, and leave analysis of the shape
of those subgraphs to later work. The source code for this project and our
profiling analysis can be found [here][source].

[fgp]: https://en.wikipedia.org/wiki/Granularity_(parallel_computing)#Fine-grained_parallelism
[llvm]: https://llvm.org
[source]: https://github.com/avanhatt/dfg-coverings


## Building data flow graphs from LLVM

- Trade-offs:
- Machine instructions vs. IR instructions
- Static vs. dynamic DFGs
- Getting simple data flow "for free" vs. complexities of control flow

## Matching DFG stencils

From a modeling perspective, a stencil is more than just the topology of the graph: a stencil also includes the class of operation for each node. For instance, consider stencil formed by the chain of instructions `pointer -> getelementptr -> load` --- the load instruction cannot be mapped arbitrarily onto other instructions: we want it to align only with program instructions which are in some sense the same. Thinking of the opcodes as each specifying a color, this makes a stencil a colored graph, and a stencil isomorphism is a bijection of colored graphs.

While graph isomorphism is a notoriously tricky problem, it is also a very common one, and we make heavy use of the `networkx.isomorphism` package, which provides tools for iterating over matches (colored graph isomorphisms) between program instructions $G$ and a stencil $H$.

We started our testing with the following hand-picked chains of instructions extracted from the `embench/matmult-int` code:

```
	chains = [
		["mul", "add", "srem"],
		["shl", "add"],
		["sdiv", "mul", "add"],
		["load", "mul"],
	]
```

Though it was never our goal to end here, it quickly became apparent that this was not going to be even a little bit effective, and would be very overfit to the program we were looking at. The original program, `matmult-int`, only matched ~4% of instructions, and other programs, such as `add.c` did not match a single one of them.



## Generating common DFG stencils

Of course, doing this by hand is tedious and not particularly effective; we would like to automate the process of finding the stencils to accelerate.

### Formal Description of the Task

If we ignore control flow, we can look at the problem purely graph theoretically. For a single trace through the program, the data flow graph $G$ is acyclic, and we would like to cover as much of it as possible with sub-graphs, corresponding to the stencils that we accelerate.
Statically, we do not know what the final data flow graph is, but we do know that we will be able to assemble one by connecting dangling edges from control-flow-free components: basic blocks.
This is the approach we take.

We would like to find a small collection of graph components $\mathcal H = \{H_i, \ldots, H_k\}$, which we can use to replace parts of and accelerate programs having basic blocks $\mathcal G = \{G_1,\ldots,G_n\}$, that maximizes the total saved time:

$$\mathcal S_{\mathcal H}(\mathcal G) := \max_{\mathcal C \in \text{Cov}(\mathcal G, \mathcal H)}~ \sum_{G \in \mathcal G} w_G \cdot  \sum_{H \in \mathcal C_G} f_H \cdot |H|$$

where:

- $\text{Cov}(\mathcal G, \mathcal H)$ is the set of all valid (partial) coverings of basic blocks with at most one stencil, that is, injective graph morphisms $\varphi: (\cup \mathcal G) \to \cup \mathcal H$.

- $\mathcal C_G$ is the component of the covering $\mathcal C$ of the total covering on the particular basic block graph $G$.

- $w_G$ is independent of $\mathcal H$ and proportional to the expected number of times $G$ is executed.

- $f_H$ is the expected speedup factor from accelerating the component $H$.

Of course, supposing that $f_H$ was roughly constant, we could trivially achieve the maximum savings by choosing $\mathcal H := \mathcal G$, there are a few problems with this:

1.  $|\mathcal H|$ is large; there are many of these sub-graphs, which makes the search process substantially less efficient.
2. Each $H_i \in \mathcal H$ is also large, making the specialized component more expensive.
3. There is now a dependency between $\mathcal H$ and $\mathcal G$, and so we need to know our program in order to build the components we use to accelerate.


In fact, only the third issue is really important; the first two are roughly heuristics which help solve it.
To cast this as a learning problem, imagine that there's some underlying distribution $\mathtt{Programs}$ of programs that people write; we can now cast our work as a solution to the optimization problem of finding

$$ \arg\max_{\mathcal H}\left( \mathop{\mathbb E}\limits_{\mathcal G\sim \texttt{Programs}}~ \mathcal S_{\mathcal H}(\mathcal G) - \text{Cost}(\mathcal H) \right)$$

where $\text{Cost}(\mathcal H)$ is the additional compilation cost incurred by $\mathcal H$, which is higher for larger graphs.
In this learning analogy, finding common DFGs for a particular collection of programs is training data.

Rather than solve this optimization problem in closed form, we optimize for heuristics (1) and (2), which are effectively regularization knobs that could be used in future work to automate the entire optimization.


### Two Implementations
We implemented (partially by accident) two separate algorithms for finding the stencils from example programs.
In both cases, the general idea is to keep a connected component and explore edges, being careful not to double count different graphs that are isomorphic but presented in different orders.


#### The Tricky Details
TODO. @ Greg


#### Mutually Exclusive Matches

To generate a _valid_ covering $\mathcal C$, we need more than simply an enumeration of all sub-graphs in a program and a way to match them: we also have to make sure the matches don't step on one another's toes---that is, we need to throw out matches until each instruction is only covered by at most a single component

Finding the optimal one is difficult: it is related to the weighted optimal scheduling problem (which [can be solved with dynamic programming](https://courses.cs.washington.edu/courses/cse521/13wi/slides/06dp-sched.pdf) in $O(n \log n)$ time, but on a general directed graph, we get an exponential factor in the branching coefficient.
Rather than solve this problem optimally in the general case, we implement the greedy biggest-first strategy, and focus instead on searching for collections of matches which have higher coverage in the first place.

### Search

Ultimately, we do not need to search the space exhaustively if we have reasonable heuristics that might cause us to believe that we're going in the right direction with certain stencils.
We can then do our search traversal in a different order, guided by the objective function.

This can be done in the form of a beam search: we only keep around the $k$ best sub-graphs in the search frontier, and at each step try to expand one to a random neighboring node. 

- Scaling? TODO: what is this?

## Static and dynamic coverage
NOTE: while the implementation is maybe best put here, we needed to describe the static coverage metric earlier to explain the search process above.

- Annotations on LLVM
- Embench benchmark suite
- Use fast stencils for slow/big applications

## Ongoing directions

- Extend to hyperblock/superblock
- Compare against dynamic DFGs
- Evaluate on accelerated hardware
- Find stencils for groups of applications
